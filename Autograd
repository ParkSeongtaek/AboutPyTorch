import torch


DEVICE = torch.device('cpu')
BATCH_SIZE = 64
INPUT_SIZE = 1000

HIDDEN_SIZE = 100
OUTPUT_SIZE = 10

x= torch.randn(BATCH_SIZE,
                INPUT_SIZE,
                dtype = torch.float,
                requires_grad= False)
y= torch.randn(BATCH_SIZE,
                OUTPUT_SIZE,
                dtype = torch.float,
                requires_grad= False)
w1= torch.randn(INPUT_SIZE,
                HIDDEN_SIZE,
                dtype = torch.float,
                requires_grad= True)
w2= torch.randn(HIDDEN_SIZE,
                OUTPUT_SIZE,
                dtype = torch.float,
                requires_grad= True)


learning_rate = 1e-6
for t in range(1,501):
    y_pred = x.mm(w1).clamp(min=0).mm(w2)   # (ReLU(x.mm(w1))).mm(w2)
    
    loss = (y_pred - y).pow(2).sum()
    if t%100 ==0:
        print("Iteration: ",t,"\t","LOSS",loss.item())
    loss.backward()

    with torch.no_grad():
        w1-= learning_rate * w1.grad
        w2-= learning_rate * w2.grad
        
        w1.grad.zero_()
        w2.grad.zero_()

'''
Result 

First Run
Iteration:  100          LOSS 789.6707153320312
Iteration:  200          LOSS 6.283302307128906
Iteration:  300          LOSS 0.07854069769382477
Iteration:  400          LOSS 0.0014802836813032627
Iteration:  500          LOSS 0.00013217404193710536

Second Run
Iteration:  100          LOSS 749.996826171875
Iteration:  200          LOSS 6.228278160095215
Iteration:  300          LOSS 0.08298936486244202
Iteration:  400          LOSS 0.0015978605952113867
Iteration:  500          LOSS 0.0001309700746787712

Third Run
Iteration:  100          LOSS 937.6288452148438
Iteration:  200          LOSS 9.420062065124512
Iteration:  300          LOSS 0.1388353556394577
Iteration:  400          LOSS 0.002696493174880743
Iteration:  500          LOSS 0.0001882780052255839


'''
